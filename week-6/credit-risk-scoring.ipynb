{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9063192f",
   "metadata": {},
   "source": [
    "In this week we are dealing with Credit Risk Scoring assingment. The main idea is to decide if the customer who is looking for a loan can get it or not. We have a dataset of historical data for all the customers who either 'returned their loan back' or 'defaulted'. Here is a definition of a 'default' in context of risk-scoring:\n",
    "\n",
    "**Credit default risk** â€“ *The risk of loss arising from a debtor being unlikely to pay its loan obligations in full or the debtor is more than 90 days past due on any material credit obligation; default risk may impact all credit-sensitive transactions, including loans, securities and derivatives.*\n",
    "\n",
    "We want to make a model that will for each new customer predict the probability of defaulting. This week is mainly focused on decision-trees and the ensemble methods, but I will also check what `LogisticRegression` has to tell about it. It never hurts.\n",
    "\n",
    "Here is everything you need to know about this dataset:\n",
    "https://github.com/gastonstat/CreditScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c261b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/gastonstat/CreditScoring/master/CreditScoring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5754217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ./data $data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05254ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03aa21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>Home</th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital</th>\n",
       "      <th>Records</th>\n",
       "      <th>Job</th>\n",
       "      <th>Expenses</th>\n",
       "      <th>Income</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>182</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Status  Seniority  Home  Time  Age  Marital  Records  Job  Expenses   \n",
       "0       1          9     1    60   30        2        1    3        73  \\\n",
       "1       1         17     1    60   58        3        1    1        48   \n",
       "2       2         10     2    36   46        2        2    3        90   \n",
       "3       1          0     1    60   24        1        1    1        63   \n",
       "4       1          0     1    36   26        1        1    1        46   \n",
       "\n",
       "   Income  Assets  Debt  Amount  Price  \n",
       "0     129       0     0     800    846  \n",
       "1     131       0     0    1000   1658  \n",
       "2     200    3000     0    2000   2985  \n",
       "3     182    2500     0     900   1325  \n",
       "4     107       0     0     310    910  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/CreditScoring.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d84e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns to lowercase\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9388156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4455 entries, 0 to 4454\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   status     4455 non-null   int64\n",
      " 1   seniority  4455 non-null   int64\n",
      " 2   home       4455 non-null   int64\n",
      " 3   time       4455 non-null   int64\n",
      " 4   age        4455 non-null   int64\n",
      " 5   marital    4455 non-null   int64\n",
      " 6   records    4455 non-null   int64\n",
      " 7   job        4455 non-null   int64\n",
      " 8   expenses   4455 non-null   int64\n",
      " 9   income     4455 non-null   int64\n",
      " 10  assets     4455 non-null   int64\n",
      " 11  debt       4455 non-null   int64\n",
      " 12  amount     4455 non-null   int64\n",
      " 13  price      4455 non-null   int64\n",
      "dtypes: int64(14)\n",
      "memory usage: 487.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using R code in the GitHub repo to assint in converting values of categorical features to something understandab;e\n",
    "#but I've c/p this from the course repo:\n",
    "\n",
    "status_values = {\n",
    "    1: 'ok',\n",
    "    2: 'default',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.status = df.status.map(status_values)\n",
    "\n",
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.home = df.home.map(home_values)\n",
    "\n",
    "marital_values = {\n",
    "    1: 'single',\n",
    "    2: 'married',\n",
    "    3: 'widow',\n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.marital = df.marital.map(marital_values)\n",
    "\n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.records = df.records.map(records_values)\n",
    "\n",
    "job_values = {\n",
    "    1: 'fixed',\n",
    "    2: 'partime',\n",
    "    3: 'freelance',\n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.job = df.job.map(job_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a96463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>seniority</th>\n",
       "      <th>home</th>\n",
       "      <th>time</th>\n",
       "      <th>age</th>\n",
       "      <th>marital</th>\n",
       "      <th>records</th>\n",
       "      <th>job</th>\n",
       "      <th>expenses</th>\n",
       "      <th>income</th>\n",
       "      <th>assets</th>\n",
       "      <th>debt</th>\n",
       "      <th>amount</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ok</td>\n",
       "      <td>9</td>\n",
       "      <td>rent</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>married</td>\n",
       "      <td>no</td>\n",
       "      <td>freelance</td>\n",
       "      <td>73</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok</td>\n",
       "      <td>17</td>\n",
       "      <td>rent</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>widow</td>\n",
       "      <td>no</td>\n",
       "      <td>fixed</td>\n",
       "      <td>48</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>default</td>\n",
       "      <td>10</td>\n",
       "      <td>owner</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>married</td>\n",
       "      <td>yes</td>\n",
       "      <td>freelance</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ok</td>\n",
       "      <td>0</td>\n",
       "      <td>rent</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>single</td>\n",
       "      <td>no</td>\n",
       "      <td>fixed</td>\n",
       "      <td>63</td>\n",
       "      <td>182</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ok</td>\n",
       "      <td>0</td>\n",
       "      <td>rent</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>single</td>\n",
       "      <td>no</td>\n",
       "      <td>fixed</td>\n",
       "      <td>46</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    status  seniority   home  time  age  marital records        job  expenses   \n",
       "0       ok          9   rent    60   30  married      no  freelance        73  \\\n",
       "1       ok         17   rent    60   58    widow      no      fixed        48   \n",
       "2  default         10  owner    36   46  married     yes  freelance        90   \n",
       "3       ok          0   rent    60   24   single      no      fixed        63   \n",
       "4       ok          0   rent    36   26   single      no      fixed        46   \n",
       "\n",
       "   income  assets  debt  amount  price  \n",
       "0     129       0     0     800    846  \n",
       "1     131       0     0    1000   1658  \n",
       "2     200    3000     0    2000   2985  \n",
       "3     182    2500     0     900   1325  \n",
       "4     107       0     0     310    910  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f673892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seniority</th>\n",
       "      <th>time</th>\n",
       "      <th>age</th>\n",
       "      <th>expenses</th>\n",
       "      <th>income</th>\n",
       "      <th>assets</th>\n",
       "      <th>debt</th>\n",
       "      <th>amount</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>763317.0</td>\n",
       "      <td>1060341.0</td>\n",
       "      <td>404382.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>1463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8703625.0</td>\n",
       "      <td>10217569.0</td>\n",
       "      <td>6344253.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>99999999.0</td>\n",
       "      <td>99999999.0</td>\n",
       "      <td>99999999.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>11140.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seniority    time     age  expenses      income      assets   \n",
       "count     4455.0  4455.0  4455.0    4455.0      4455.0      4455.0  \\\n",
       "mean         8.0    46.0    37.0      56.0    763317.0   1060341.0   \n",
       "std          8.0    15.0    11.0      20.0   8703625.0  10217569.0   \n",
       "min          0.0     6.0    18.0      35.0         0.0         0.0   \n",
       "25%          2.0    36.0    28.0      35.0        80.0         0.0   \n",
       "50%          5.0    48.0    36.0      51.0       120.0      3500.0   \n",
       "75%         12.0    60.0    45.0      72.0       166.0      6000.0   \n",
       "max         48.0    72.0    68.0     180.0  99999999.0  99999999.0   \n",
       "\n",
       "             debt  amount    price  \n",
       "count      4455.0  4455.0   4455.0  \n",
       "mean     404382.0  1039.0   1463.0  \n",
       "std     6344253.0   475.0    628.0  \n",
       "min           0.0   100.0    105.0  \n",
       "25%           0.0   700.0   1118.0  \n",
       "50%           0.0  1000.0   1400.0  \n",
       "75%           0.0  1300.0   1692.0  \n",
       "max    99999999.0  5000.0  11140.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The missing values (based on data description) are values of type 99999.0000\n",
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5966d1",
   "metadata": {},
   "source": [
    "This is an useful trick. To find which one have missing values, you do describe and see the max values.\n",
    "When you fix them to NaNs then \"the next maximum value\" will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128c53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['income', 'assets', 'debt']:\n",
    "    df[c] = df[c].replace(to_replace=99999999, value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3fff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seniority</th>\n",
       "      <th>time</th>\n",
       "      <th>age</th>\n",
       "      <th>expenses</th>\n",
       "      <th>income</th>\n",
       "      <th>assets</th>\n",
       "      <th>debt</th>\n",
       "      <th>amount</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4421.0</td>\n",
       "      <td>4408.0</td>\n",
       "      <td>4437.0</td>\n",
       "      <td>4455.0</td>\n",
       "      <td>4455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5403.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>1463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>11573.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>11140.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seniority    time     age  expenses  income    assets     debt  amount   \n",
       "count     4455.0  4455.0  4455.0    4455.0  4421.0    4408.0   4437.0  4455.0  \\\n",
       "mean         8.0    46.0    37.0      56.0   131.0    5403.0    343.0  1039.0   \n",
       "std          8.0    15.0    11.0      20.0    86.0   11573.0   1246.0   475.0   \n",
       "min          0.0     6.0    18.0      35.0     0.0       0.0      0.0   100.0   \n",
       "25%          2.0    36.0    28.0      35.0    80.0       0.0      0.0   700.0   \n",
       "50%          5.0    48.0    36.0      51.0   120.0    3000.0      0.0  1000.0   \n",
       "75%         12.0    60.0    45.0      72.0   165.0    6000.0      0.0  1300.0   \n",
       "max         48.0    72.0    68.0     180.0   959.0  300000.0  30000.0  5000.0   \n",
       "\n",
       "         price  \n",
       "count   4455.0  \n",
       "mean    1463.0  \n",
       "std      628.0  \n",
       "min      105.0  \n",
       "25%     1118.0  \n",
       "50%     1400.0  \n",
       "75%     1692.0  \n",
       "max    11140.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca33ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is another clever trick to sift out that one 'unk' value for 'status'\n",
    "df = df[df.status != 'unk'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d492e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239e44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseting the index of them since they've shuffeled so when you finally sift the y values you get no confusion\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5272aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'defaults' should be 1's because we are making a model to predict those who will potentialy 'default'\n",
    "y_train = (df_train.status == 'default').astype('int').values\n",
    "y_val = (df_val.status == 'default').astype('int').values\n",
    "y_test = (df_test.status == 'default').astype('int').values\n",
    "\n",
    "#Now that we collected all of them it is time to delete it from the feature matrix so that we don't introduce data-leaks\n",
    "del df_train['status']\n",
    "del df_val['status']\n",
    "del df_test['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703a2a1",
   "metadata": {},
   "source": [
    "## Training and Evaluating the DecisionTree using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fca015c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39d10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9372592c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status        0\n",
       "seniority     0\n",
       "home          0\n",
       "time          0\n",
       "age           0\n",
       "marital       0\n",
       "records       0\n",
       "job           0\n",
       "expenses      0\n",
       "income       34\n",
       "assets       47\n",
       "debt         18\n",
       "amount        0\n",
       "price         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how many NaNs are in the whole dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2c5111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precentage of NaNs in a dataset\n",
    "round((df.isna().sum().sum()*100/len(df)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a0449",
   "metadata": {},
   "source": [
    "DecisionTrees can't handle NaNs since they are assesing splits using Information Gain. At this point, instructor probably has a good reason (for now) that he filled those NaNs with 0. If you ask me, since it is only 2.22% of the dataset I would delete those rows. But, also wont exclude the possibility that I can use some clever back-of-the-envelope calculations to asses if I can fix them. Maybe, there is a possibility of making them stay NaNs. ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54b8f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it is called Dict Vectorizer it means we have to convert the dataset into dicitionary\n",
    "#before vectorizing it \n",
    "\n",
    "train_dicts = df_train.fillna(0).to_dict(orient='records')\n",
    "val_dicts = df_val.fillna(0).to_dict(orient='records')\n",
    "\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_valid = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64fd4017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's train plain Decision Tree on the training dataset \n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e29311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510999347815181"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's asses the AUC score on the y_val, but \n",
    "\n",
    "y_pred = dt.predict_proba(X_valid)[:, 1]\n",
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a86f56",
   "metadata": {},
   "source": [
    "At this point, you might be thinking 'such poor score it is just slightly better than Random Classifier!', but here there is a *trick* to explain it, and it will also explain why `predict_proba` gives you only ones and zeroes. \n",
    "\n",
    "Let's test the AUC score on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1facb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999996473061242"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's test the AUC score on the train dataset\n",
    "y_pred = dt.predict_proba(X_train)[:, 1]\n",
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6dd0b",
   "metadata": {},
   "source": [
    "AUC of nearly 1? That is an ideal model! But the problem is that decision trees that have no 'prunning' will *always* overfit the training data, because for each instance in the dataset they will find sspecific rules (watch the intrudoctory video). \n",
    "\n",
    "It is not a problem of a model, since it wants to find the best score possible and in that it learns specific rules such that for each datapoint it gives perfect score. The more you have specific rules, the more you are good at memorizing the stuff rather than generalizing it. When you are memorizing each datapoint, yes it will be good at recognizing those, but how about new data? You can't just keep memorizing. That model is useless! You are basically training a model that for given input (dataset) will output perfect predictions only if your new data matches one of the rows in the dataset. That problem is called overfiting. \n",
    "\n",
    "So, after knowing that we are overfitting and for each datapoint we have a particular basket then why do predict proba gives 0s and 1s?\n",
    "\n",
    "Decision trees are inherently non-probabilistic models. Unlike some other algorithms (e.g., logistic regression), decision trees don't naturally output probabilities. \n",
    "\n",
    "Instead `predict_proba` returns the fraction of samples of each class in the leaf node where a particular instance ends up. This is more of a distribution of class counts rather than true probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7412a73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  1.0  0.0\n",
       "1  1.0  0.0\n",
       "2  1.0  0.0\n",
       "3  0.0  1.0\n",
       "4  0.0  1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dt.predict_proba(X_valid)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb7f52",
   "metadata": {},
   "source": [
    "Let's make the tree grow up to depth of level 3 and see how good it 'generalizes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e09b715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4008e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.774310483472765\n",
      "val: 0.7455663974313953\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt.predict_proba(X_train)[:, 1]\n",
    "auc = roc_auc_score(y_train, y_pred)\n",
    "print('train:', auc)\n",
    "\n",
    "y_pred = dt.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "print('val:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6191373",
   "metadata": {},
   "source": [
    "Since there is no much of a difference between train and validation AUC score, we see that it generalizes pretty good. Let's see the rules it learned along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fc0b05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- seniority <= 2.50\n",
      "|   |--- records=yes <= 0.50\n",
      "|   |   |--- job=fixed <= 0.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- job=fixed >  0.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- records=yes >  0.50\n",
      "|   |   |--- income <= 136.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- income >  136.50\n",
      "|   |   |   |--- class: 1\n",
      "|--- seniority >  2.50\n",
      "|   |--- records=yes <= 0.50\n",
      "|   |   |--- income <= 93.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |   |--- income >  93.50\n",
      "|   |   |   |--- class: 0\n",
      "|   |--- records=yes >  0.50\n",
      "|   |   |--- assets <= 3450.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- assets >  3450.00\n",
      "|   |   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "print(export_text(dt, feature_names=dv.feature_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1bbbeb",
   "metadata": {},
   "source": [
    "Let's give `max_depth==2` for the sake of simplicity in understanding these 'if-else' decisions DT learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbe5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.7175133670978937\n",
      "val: 0.7128913108914865\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict_proba(X_train)[:, 1]\n",
    "auc = roc_auc_score(y_train, y_pred)\n",
    "print('train:', auc)\n",
    "\n",
    "y_pred = dt.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "print('val:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58a9daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- seniority <= 2.50\n",
      "|   |--- records=yes <= 0.50\n",
      "|   |   |--- class: 0\n",
      "|   |--- records=yes >  0.50\n",
      "|   |   |--- class: 1\n",
      "|--- seniority >  2.50\n",
      "|   |--- records=yes <= 0.50\n",
      "|   |   |--- class: 0\n",
      "|   |--- records=yes >  0.50\n",
      "|   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(export_text(dt, feature_names=dv.feature_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860d9f4",
   "metadata": {},
   "source": [
    "Out of all these let's focus on how would you interpret rules like `records=yes <= 0.50`.\n",
    "\n",
    "`records` is a binary variable with values 'yes' and 'no':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88d3e944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "records\n",
       "no     3681\n",
       "yes     773\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.records.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592bc67",
   "metadata": {},
   "source": [
    "This is the thing that might be confusing ex. `records=yes <= 0.5`. \n",
    "\n",
    "Given that `records=yes` is `True` and we map `True` to `1`, then `records=yes <= 0.5` would mean `True or 1 <=1` aka `records=yes and it is the opposite of that` (so effectively it is `records=no`), whilst `records=yes > 0.5` would mean `records=yes and it is right that`. \n",
    "\n",
    "--- \n",
    "\n",
    "At this point the Alexey is explaining the simple implementation of Decision Tree algorithm (this course is a refresher for me!):\n",
    "\n",
    "- For each column name all the Thresholds.\n",
    "- For each threshold split the column into two parts (left and right) and check impurity for left and right. For each 'side' if there are less elements of class A than class B and vice versa, then those are 'impurities' so we asses percentage of them. \n",
    "- We keep the impurity score for each of the threshold both for 'left' and 'right' split, and summarize them into one value (simple average is sufficient for this very simple implementation)\n",
    "- After one column is finished we do the same process for the next column, and next and next etc. \n",
    "- Then we make the first split based on the best avg. impurity. \n",
    "- We are left with 'left' and 'right' sid'. Now, with less data and again with all the columns, for each side we recursively do the same process IF they are not pure, IF we have't reached `max_depth` and IF we have a limit of number of elements per class for each split (`max_samples_leaf`). \n",
    "\n",
    "\n",
    "For tuning the model Alexey uses a slightly economic variant of Grid Search. He trains Decision Tree for max depth, looks for ~5 values that give best scores and then given those values finds best `max_samples_leaf`. His approach is more visual and I do like it, but in a nutshell I would keep using GridSearch since DecisionTrees are fairly fast to train.  \n",
    "\n",
    "Let's test the GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef93dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_values = [2, 3, 4, 5, 6, 10, 15, 20]\n",
    "max_samples_per_leaf_values = [1, 2, 5, 10, 15, 20, 50, 100, 200, 300] #The bigger values are there because of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a1e819c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=DecisionTreeClassifier(max_depth=2, random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5, 6, 10, 15, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 5, 10, 15, 20, 50, 100, 200,\n",
       "                                              300]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=DecisionTreeClassifier(max_depth=2, random_state=42),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5, 6, 10, 15, 20],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 5, 10, 15, 20, 50, 100, 200,\n",
       "                                              300]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=DecisionTreeClassifier(max_depth=2, random_state=42),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 10, 15, 20],\n",
       "                         'min_samples_leaf': [1, 2, 5, 10, 15, 20, 50, 100, 200,\n",
       "                                              300]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': max_depth_values,\n",
    "    'min_samples_leaf': max_samples_per_leaf_values\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f9e353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 50}\n",
      "Best AUC: 0.80\n"
     ]
    }
   ],
   "source": [
    "#Print the best parameters and corresponding AUC score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best AUC: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e87fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.78\n"
     ]
    }
   ],
   "source": [
    "#Using the best model let's test the AUC score\n",
    "best_model = grid_search.best_estimator_\n",
    "test_auc = roc_auc_score(y_val, best_model.predict_proba(X_valid)[:, 1])\n",
    "print(\"Test AUC: {:.2f}\".format(test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8cf35",
   "metadata": {},
   "source": [
    "We see that it is not overfitting and that it gives decent AUC. Finally, lets train it on `df_full_train` and test it on `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79b1aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the full_train\n",
    "df_full_train = df_full_train.reset_index(drop=True)\n",
    "y_full_train = (df_full_train.status == 'default').astype('int').values\n",
    "\n",
    "del df_full_train['status']\n",
    "\n",
    "full_train_dicts = df_full_train.fillna(0).to_dict(orient='records')\n",
    "X_full_train = dv.transform(full_train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "874d4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, min_samples_leaf=50,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, min_samples_leaf=50,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=50,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the moel on full_train\n",
    "best_model.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16e926f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The test dataset is already prepared\n",
    "test_dicts = df_test.fillna(0).to_dict(orient='records')\n",
    "X_test = dv.transform(test_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dc586d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.80\n"
     ]
    }
   ],
   "source": [
    "test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "print(\"Test AUC: {:.2f}\".format(test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9328994",
   "metadata": {},
   "source": [
    "As expected since we've trained on more data! Does the model predicts the probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31215b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50943396, 0.49056604],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8627451 , 0.1372549 ],\n",
       "       ...,\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.98245614, 0.01754386],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c1f41",
   "metadata": {},
   "source": [
    "No, they look like probabilities but in an essence Decision Trees are not probabilistic models. Do they base any of their decision based on probability? Well, the Gini Impurity score is based on the probability but more or less thinking of it as 'impurity' to decide which feature has best treshold. \n",
    "\n",
    "Here is a quick reading: \n",
    "https://medium.com/ml-byte-size/how-does-decision-tree-output-predict-proba-12c78634c9d5\n",
    "\n",
    "\n",
    "What is left in those leaves after training is proportion of 0s and 1s. Maybe could be interpreted as a proxy for probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d57259",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "In a nutshell: Instead of one tree lets have a board of members, different individuals who are basing their result on their 'unique' characteristics. If they have the same characteristics then the model would not be that interesting and why would someone hire 5 people with the same opinion if one would suffice? How to make decision trees more 'individual' in nature? Train them on different features (randomly chosen), possibly with different subsets of data to make them more 'individual'. Interestingly enough, by increasing variability we are getting more and more 'precise' so to speak. After everyone gives an opinion, we take an average. Since we take those 'proxies of probabilities' into account, then averaging them gives you 'true probability'. As for assesing any probability estimate the more you take into account when taking average, by the law of large numbers, the average would converge to 'true probability', and hence after having more trees than ex. 200 the result would keep stagnating. \n",
    "\n",
    "Again, I will use GridSearchCV to find the best model although I strongly recommend you if you didn't take first course in ML to watch the videos and go through the process like Alexey did it. You will gain a better understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57844f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9474fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [20, 50, 100, 200],\n",
    "    'max_depth': max_depth_values,\n",
    "    'min_samples_leaf': max_samples_per_leaf_values,\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False] #if False whole dataset is used to train each tree, and vice versa\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1827b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit\n",
    "#grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69039b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, min_samples_leaf=50, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, min_samples_leaf=50, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=50, random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To not retrain it each time we start this ipynb uncomment the cell above. Here is what I've found:\n",
    "best_model = RandomForestClassifier(max_depth=10, min_samples_leaf=50,\n",
    "                       n_estimators=100, max_features='sqrt', random_state=RANDOM_STATE)\n",
    "\n",
    "best_model.fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa88746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.83\n"
     ]
    }
   ],
   "source": [
    "test_dicts = df_test.fillna(0).to_dict(orient='records')\n",
    "X_test = dv.transform(test_dicts)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "print(\"Test AUC: {:.2f}\".format(test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e92af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.85\n"
     ]
    }
   ],
   "source": [
    "#Is it overfitting?\n",
    "\n",
    "test_auc = roc_auc_score(y_val, best_model.predict_proba(X_valid)[:, 1])\n",
    "print(\"Test AUC: {:.2f}\".format(test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b85a0",
   "metadata": {},
   "source": [
    "Slightly better than pure Decision Tree. Finally let's train the XGBoost. \n",
    "\n",
    "## Gradient Boosted Trees (AnyBoost)\n",
    "\n",
    "I felt an urge to revisit how gradient boosted trees work and Boosting in particular because I so much like the idea. No lectures, blogs, videos etc. gave me a click until I saw awesome lecture by amazing professor Kilian Q. Weinberger. If you are just a bit math inclined, and you have some math matiruity, and you have freshly in your head concepts of inner product, thinking of functions as vectors in n-dimensional space look no further than this:\n",
    "\n",
    "https://www.youtube.com/watch?v=dosOtgSdbnY\n",
    "\n",
    "Even if you don't know them well, you will possibly gain a good intuition. What I will present here is a bit 'decoded' version of the lecture, for my future reference, and what I think will be of much importance to understand it even better.\n",
    "\n",
    "### Dot-product revisited\n",
    "\n",
    "First, the professor is drawing a line (hyperplane) which is orthogonal to a vector. Why? There are two versions of interpreting dot product (inner product) for case of a 2D:\n",
    "\n",
    "a) Given two vectors u and w, you want to find projection of u onto w. You draw a line through the vector w, and project the vector u orthogonally onto that line and calculate the the projection. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{proj}_{\\mathbf{w}}(\\mathbf{u}) = \\frac{\\mathbf{u} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2} \\cdot \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "b) Given the same two vectors, you draw a line perpendicular to a vector u. Now you project vector w onto that line and the dot product becomes the usual (Polar) version:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{u} \\cdot \\mathbf{w} = \\|\\mathbf{u}\\| \\cdot \\|\\mathbf{w}\\| \\cdot \\cos(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In both cases you get a scalar value, and that scalar value tells you if they are pointing in the same direction (dot_product>0), opposite direction (dot_product<0) or they are orthogonal to one another (dot_product=0). \n",
    "\n",
    "Refer to this if you feel a bit rusty on it: https://www.youtube.com/watch?v=LyGKycYT2v0\n",
    "\n",
    "\n",
    "### They always converge. Why?\n",
    "\n",
    "To give the intuition of 'why should boosting always converge towards y_true' he draws something like this:\n",
    "\n",
    "<img src=\"./imgs/pythagoras_v2.png\" width = \"30%\"></img>\n",
    "\n",
    "Think of this 'sequential' stuff similar to when you walk in snow and leaving footprints for someone behind you to get his/her shoes less wet. Think of walking towards where each vector points to and stop there. Wait till new weak learner, which will give you 'new predictions', appears and then walk there etc.\n",
    "\n",
    "Say that you already trained some weak learners and you get to some point which presents 'the latest predictions of latest weak learner' (red point). That is your starting point. Now, the whole idea (as I will later explain) is that from that point you train a hypothetical next weak learner such that the dot product between vector pointing towards y vector (which are true values of y) AND the vector (next weak learner which will lead you to new predictions) is bigger than 0. That is one of the reasons why it will eventually converge. The whole idea is to find such inner-product between those two vectors to minimize the loss. So we draw an arbitrary vector which satisfies that condition and presents 'next weak learner' or, should I say, 'updated predictions of next weak learner'. \n",
    "\n",
    "Given the 'present predictions of latest weak learner' and some arbitrary new weak learner which will give you 'next predictions' which satisfies the idea of pointing in the same direction as vector that points towards y_true we can draw a right triangle. Say we have a weak learner that overshoots to some point. We draw a right triangle, containing 'starting point', 'point which presents predictions of next weak learner' and the point that presents 'true labels'.\n",
    "\n",
    "Since weak learner gives predictions which could potentially 'overshoot' (the vector has huge magnitude) we only trust it some percent (which is the learning rate!). Say you have a vector of magnitude 10. Multiply each component of a vector with some value <1 (which is learning rate) and the vector will shrink. \n",
    "\n",
    "The present hypotenuse (green one) is the magnitude of vector from the current weak learner to the true labels. If on the same triangle we 'move' towards blue point which represents 'hypothetical' new weak learner (vector that shinked using learning rate smaller than 1) then a^2 stays the same, b^2 is smaller and if we moved towards the blue point (next weak learner) then we would have 'new (updated) hypotenuse/purple one' which is smaller in magnitude than the previous one!\n",
    "\n",
    "If we keep repeating the same process, satisfying the condition for dot (inner) product, then eventually we will converge towards y_true. Draw it on paper by satisfying the condition each step and you will see that you will always converge toward the point of y_true.\n",
    "\n",
    "What if we have negative dot product? That means they are pointing in the opposite direction. If that is the case we just flip the vector and they will point to the same direction!\n",
    "\n",
    "What if we have case when dot product is 0? Well, we can't do any tricks in that case. The vectors are orthogonal and the algorithm stops. Maybe nudge the vector in some direction and check the dot product?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The premise: Can weak learners (H) be combined to generate a strong learner with low bias. \n",
    "\n",
    "Here is some basic intuition that might guide you well whilst watching the video. Imagine an n-dimensional space where each axis represents labels of your training dataset. For the simplicity sake think in 3D. Think for a moment that the axis are real numbers but there is only one number which shows true label. Let's introduce the vector y_true which represents true label for each datapoint (each axis), be it binary classification or regression problem. Imagine that vector as one pointing to a datapoint in n-dimensional space. How would we get to that point by \"combining\" weak learners? We started with a premise that we want a lot of weak learners to be combined to generate a strong one. \n",
    "\n",
    "Idea of combining might be at first guess something to do with \"adding\", and we will see later why it is 'sequential'. When you have a very weak learner (something slightly better than random guessing) it will obviously predict something bad and not at all close to true values. Imagine it as a decision stump. So, you train a model and it produces some y_predicted values. Now, try to visualize the vector having values of y_predicted. You don't expect from weak learner to be 'perfect' so vector y_predicted and y_true are not close, maybe they even point out at different directions. \n",
    "\n",
    "What matters is the idea of 'trusting those predictions' which is our learning rate or `eta`. If you have a vector and multiply it with a scalar value <1 you will see that it's magnitude will change and it will 'shrink' (sometimes learning rate is called 'shrinkage'). Starting from that how do we find next tree? First of all remember that each time you find a predictions, that very datapoint will serve us as a center of a NEW coordinate system. That will be our 'new starting point' (refer to the 'footsteps in a snow from the above').\n",
    "\n",
    "To find the next weak learner (we already trained one!) we expect from this starting point to have a next one such that the loss between new weak learner's predictions and true values are small. In some sense we want to 'guide' each new weak learners prediction vector towards the actual values. Here is where a math gets little bit messy, but if you watched the video till the end you would find out that loss presents vector that points out from true values to our starting point. Basically we need to minimize the inner product between vector H(xi)-y_true (where H(xi) are all the week learners we added up until this moment) AND h(xi) the weak learner we want to train. \n",
    "\n",
    "Obviously, the vector H(xi)-y_true points towards our starting point because that is the idea of vector subtraction. So, in order to minimize the inner-product it has to be more and more 'negative'. Given our 'starting point' and the vector pointing towards the starting point, where should our next vector point to? \n",
    "Remember that dot product is negative if they point in opposite directions? Well, to make inner-product negative they have to point in the opposite direction so the 'new' vector should always point on the opposite of the vector H(xi)-y_true to keep inner-product more negative. \n",
    "\n",
    "How I think that algorithm works internaly at this point: given all the combinations of features and thresholds to split it and make a decision tree, find the weak learner that gives predictions such that inner-product between those predictions and the 'compass' vector H(xi)-y_true is minimum. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "After a while you will see that this kind of pattern reminds you of 'gradient descent'. Say you have a random function. If you don't have any objective in mind then when you take a gradient of a function at some random point it will point you to something (either maximum or minimum). But if the objective is to minimize something it does not necessarily means that it has to be zero. In both cases be it finding minima or 'the most negative point' you are doing a bunch of steps. \n",
    "\n",
    "Oh, and the 'gradient' in gradient boosting refers to residual. If you ever seen linear regression then you will see that residual is defined as `actual-predicted` which is same as H(xi)-y. \n",
    "\n",
    "If the learning rate is big there is a high chance of overshooting or that algo keeps diverging instead of converging towards y_true. \n",
    "If the learning rate is small then we are taking baby-steps but maybe we stop in the middle of the road towards y_true? Then we need more weak learners (increase the number of boosting rounds). \n",
    "\n",
    "---\n",
    "\n",
    "What I will do next is train plain XGBoost to see if everything is set properly, but I will not do GridSearchCV because there are too much parameters to learn and the GridSearchCV started to feel a little dull to use. Imagine how much time it would take to test different permutations for GBT's. Knowing that such problem is arising, the course instructor suggested me to look for the MLFlow in the MLOps Zoomcamp. I will train the basic model here and use the best parameters that instructor obtained. Then I will detach to learn a bit about MLFlow and then come back to this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d2a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
